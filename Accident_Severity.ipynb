{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import (svm, preprocessing)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (recall_score, precision_score, accuracy_score, confusion_matrix,)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"./Data/Accident_train.csv\")\n",
    "data_test = pd.read_csv(\"./Data/Accident_test.csv\")\n",
    "#data_train.head()\n",
    "#data_test.head()\n",
    "#data_train.info()\n",
    "#data_test.info()\n",
    "#data_train.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    8529\n",
       "3.0      26\n",
       "2.0      18\n",
       "Name: Ped_Crossing_HC, dtype: int64"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_train[\"Weekday_of_Collision\"].value_counts()\n",
    "#data_train[\"Policing_Area\"].value_counts()\n",
    "#data_test[\"Policing_Area\"].value_counts()\n",
    "#data_test[\"Collision_Severity\"].value_counts()\n",
    "#data_test[\"Weekday_of_Collision\"].value_counts()\n",
    "data_train[\"Ped_Crossing_HC\"].value_counts()\n",
    "#print(type(data_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# lable encoder for data_train\n",
    "le.fit(data_train['Weekday_of_Collision'])\n",
    "data_train['Weekday_of_Collision'] = le.transform(data_train['Weekday_of_Collision'])\n",
    "le.fit(data_train['Policing_Area'].astype(str))\n",
    "data_train['Policing_Area'] = le.transform(data_train['Policing_Area'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Nan Values to mean/mode etc.,\n",
    "#data_train.isna().any()\n",
    "#data_train.mean()\n",
    "#data_train.isna().any()\n",
    "data_train = data_train.fillna(data_train.median())\n",
    "# Taking care of missing values with most frequent values\n",
    "#data_train = data_train.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lable encoder for data_test\n",
    "le.fit(data_test['Collision_Severity'])\n",
    "data_test['Collision_Severity'] = le.transform(data_test['Collision_Severity'])\n",
    "le.fit(data_test['Weekday_of_Collision'])\n",
    "data_test['Weekday_of_Collision'] = le.transform(data_test['Weekday_of_Collision'])\n",
    "le.fit(data_test['Policing_Area'].astype(str))\n",
    "data_test['Policing_Area'] = le.transform(data_test['Policing_Area'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Nan Values to mean/mode etc.,\n",
    "#data_test.isna().any()\n",
    "#data_test.mean()\n",
    "data_test = data_test.fillna(data_test.median())\n",
    "#data_train.isna().any()\n",
    "#data_test = data_test.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = data_train[\"Collision_Severity\"]\n",
    "data_train_x = data_train.drop([\"Collision_Severity\", \"Collision_Ref_No\"], axis=1)\n",
    "\n",
    "data_test_x = data_test.drop([\"Collision_Severity\", \"Collision_Ref_No\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8849, 15)"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# splitting test vales into parts to find get lables\n",
    "data_variables = train_test_split(data_train_x, labels_train, test_size = 0.25, random_state = 42)\n",
    "#data_variables = train_test_split(data_train, labels_train, test_size=0.33, random_state=99)\n",
    "data_train_l, data_test_l, labels_train_l, labels_test_l = data_variables\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data_train_l)\n",
    "train_data_scaled = scaler.transform(data_train_l)\n",
    "test_data_scaled = scaler.transform(data_test_l)\n",
    "\n",
    "test_data_scaled_x = scaler.transform(data_test_x)\n",
    "\n",
    "#pca = PCA(n_components = 14)\n",
    "#train_data_scaled = pca.fit_transform(data_train_l)\n",
    "#test_data_scaled = pca.transform(data_test_l)\n",
    "#explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "#test_data_scaled_x = pca.transform(data_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({3: 2169, 2: 43, 1: 1})\n",
      "accuracy  : 0.9023949389968369\n",
      "accuracy[round]  : 0.902\n",
      "precision : 0.9023949389968369\n",
      "recall    : 0.9023949389968369 \n",
      "\n",
      "Confusion matrix \n",
      "[[   0    6   21]\n",
      " [   1   27  178]\n",
      " [   0   10 1970]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#rf = RandomForestRegressor(n_estimators = 1000, random_state = 4)\n",
    "                    \n",
    "classifier = XGBClassifier()\n",
    "classifier.fit(train_data_scaled, labels_train_l)\n",
    "\n",
    "classifier.fit(train_data_scaled, labels_train_l)\n",
    "\n",
    "# Predicting test data with train data using classifier\n",
    "predict_y = classifier.predict(test_data_scaled)\n",
    "\n",
    "#print(type(predict_y))\n",
    "a = np.array(predict_y)\n",
    "print(collections.Counter(a))\n",
    "#for x in predict_y:\n",
    "#  print(x)\n",
    "\n",
    "accuracy = classifier.score(test_data_scaled, labels_test_l)\n",
    "\n",
    "precision = precision_score(labels_test_l, predict_y, average='micro')\n",
    "recall = recall_score(labels_test_l, predict_y, average='micro')\n",
    "\n",
    "cmatrix = confusion_matrix(labels_test_l, predict_y)\n",
    "print(\"accuracy  : {}\".format(accuracy))\n",
    "print(\"accuracy[round]  : {}\".format(round(accuracy, 3)))\n",
    "print(\"precision : {}\".format(precision))\n",
    "print(\"recall    : {} \\n\".format(recall))\n",
    "print(\"Confusion matrix \\n{}\\n\\n\".format(cmatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>X Boost</td>\n",
       "      <td>90.24</td>\n",
       "      <td>{3: 2169, 2: 43, 1: 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>89.70</td>\n",
       "      <td>{3: 2185, 2: 28}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>89.47</td>\n",
       "      <td>{3: 2213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>89.47</td>\n",
       "      <td>{3: 2213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>89.47</td>\n",
       "      <td>{3: 2213}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>88.39</td>\n",
       "      <td>{3: 2183, 1: 24, 2: 6}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>87.08</td>\n",
       "      <td>{3: 2099, 2: 86, 1: 28}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>85.59</td>\n",
       "      <td>{3: 1938, 2: 242, 1: 33}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>81.11</td>\n",
       "      <td>{3: 1943, 2: 270}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>10.12</td>\n",
       "      <td>{1: 1974, 3: 200, 2: 39}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Score                     Count\n",
       "9                     X Boost  90.24    {3: 2169, 2: 43, 1: 1}\n",
       "3               Random Forest  89.70          {3: 2185, 2: 28}\n",
       "0     Support Vector Machines  89.47                 {3: 2213}\n",
       "2         Logistic Regression  89.47                 {3: 2213}\n",
       "7                  Linear SVC  89.47                 {3: 2213}\n",
       "6  Stochastic Gradient Decent  88.39    {3: 2183, 1: 24, 2: 6}\n",
       "1                         KNN  87.08   {3: 2099, 2: 86, 1: 28}\n",
       "8               Decision Tree  85.59  {3: 1938, 2: 242, 1: 33}\n",
       "5                  Perceptron  81.11         {3: 1943, 2: 270}\n",
       "4                 Naive Bayes  10.12  {1: 1974, 3: 200, 2: 39}"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVC\n",
    "svc = SVC()\n",
    "svc.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = svc.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_svc = collections.Counter(a)\n",
    "acc_svc = round(svc.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_svc\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = knn.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_knn = collections.Counter(a)\n",
    "acc_knn = round(knn.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_knn\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = logreg.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_log = collections.Counter(a)\n",
    "acc_log = round(logreg.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_log\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = gaussian.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_gaussian = collections.Counter(a)\n",
    "acc_gaussian = round(gaussian.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_gaussian\n",
    "\n",
    "# Perceptron\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = perceptron.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_perceptron = collections.Counter(a)\n",
    "acc_perceptron = round(perceptron.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_perceptron\n",
    "\n",
    "# Linear SVC\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = linear_svc.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_linear_svc = collections.Counter(a)\n",
    "acc_linear_svc = round(linear_svc.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_linear_svc\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = sgd.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_sgd = collections.Counter(a)\n",
    "acc_sgd = round(sgd.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_sgd\n",
    "\n",
    "# Decision Tree\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = decision_tree.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_decision_tree = collections.Counter(a)\n",
    "acc_decision_tree = round(decision_tree.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_decision_tree\n",
    "\n",
    "# Random Forest\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = random_forest.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_random_forest = collections.Counter(a)\n",
    "acc_random_forest = round(random_forest.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_random_forest\n",
    "\n",
    "# XG Boost\n",
    "xgboost = XGBClassifier()\n",
    "xgboost.fit(train_data_scaled, labels_train_l)\n",
    "Y_pred = xgboost.predict(test_data_scaled)\n",
    "a = np.array(Y_pred)\n",
    "predict_xgboost = collections.Counter(a)\n",
    "acc_xgboost = round(xgboost.score(test_data_scaled, labels_test_l) * 100, 2)\n",
    "acc_xgboost\n",
    "\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree', 'X Boost'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree, acc_xgboost],\n",
    "    'Count': [predict_svc, predict_knn, predict_log, \n",
    "              predict_random_forest, predict_gaussian, predict_perceptron, \n",
    "              predict_sgd, predict_linear_svc, predict_decision_tree, predict_xgboost]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Accident_Severity.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
